{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachel-0420/TLM-Continuous-Data/blob/main/Example_TLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TheLastMetric end-to-end\n",
        "\n",
        "_Alex Malz (CMU), Bryan Scott (Northwestern), FranÃ§ois Lanusse (CEA), John Franklin Crenshaw (UW), Melissa Graham (UW)_"
      ],
      "metadata": {
        "id": "LhSp65FtVark"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGuNECrstsTE",
        "outputId": "d4b9ea9d-341e-497a-c30a-9971e0f647be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda] in /usr/local/lib/python3.10/dist-packages (0.4.28)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda]) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax[cuda]) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda]) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax[cuda]) (1.11.4)\n",
            "Requirement already satisfied: jaxlib==0.4.28+cuda12.cudnn89 in /usr/local/lib/python3.10/dist-packages (from jax[cuda]) (0.4.28+cuda12.cudnn89)\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.10/dist-packages (5.3.4)\n",
            "Requirement already satisfied: pzflow in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: corner in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from astropy) (1.25.2)\n",
            "Requirement already satisfied: pyerfa>=2.0 in /usr/local/lib/python3.10/dist-packages (from astropy) (2.0.1.4)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from astropy) (6.0.1)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from astropy) (24.0)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from pzflow) (0.3.8)\n",
            "Requirement already satisfied: jax>=0.4.16 in /usr/local/lib/python3.10/dist-packages (from pzflow) (0.4.28)\n",
            "Requirement already satisfied: jaxlib>=0.4.16 in /usr/local/lib/python3.10/dist-packages (from pzflow) (0.4.28+cuda12.cudnn89)\n",
            "Requirement already satisfied: optax>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from pzflow) (0.2.2)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.10/dist-packages (from pzflow) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pzflow) (4.66.4)\n",
            "Requirement already satisfied: matplotlib>=2.1 in /usr/local/lib/python3.10/dist-packages (from corner) (3.7.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->pzflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->pzflow) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->pzflow) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1->corner) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1->corner) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1->corner) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1->corner) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1->corner) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1->corner) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1->corner) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax>=0.1.4->pzflow) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax>=0.1.4->pzflow) (0.1.86)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->pzflow) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->pzflow) (2024.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax>=0.1.4->pzflow) (4.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax>=0.1.4->pzflow) (0.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1->corner) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# install required libraries\n",
        "\n",
        "!pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install astropy pzflow corner"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports all of the dependencies for training and working with normalizing flows"
      ],
      "metadata": {
        "id": "fWa3j1RF2s0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import statements\n",
        "\n",
        "from pzflow import Flow\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import corner\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from astropy.table import Table\n",
        "from pzflow import Flow, FlowEnsemble\n",
        "from pzflow.distributions import Uniform\n",
        "from pzflow.bijectors import Chain, StandardScaler, NeuralSplineCoupling, ColorTransform, InvSoftplus, RollingSplineCoupling, ShiftBounds\n",
        "from collections import namedtuple\n",
        "\n",
        "import scipy.stats as sps"
      ],
      "metadata": {
        "id": "cjMKwoiyt9q7",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6_F-vU24h1di",
        "outputId": "dba5f7e1-6383-44e7-a7f0-69c7f4f08eac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepend = '/content/drive/MyDrive/'\n",
        "input_data_dir = '/content/drive/MyDrive/TLMdata/'\n",
        "data_dir = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "oGoYYYB5-aQP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell opens a readme file containing the mapping between simulation runids and observing strategy names."
      ],
      "metadata": {
        "id": "b0D0kg3P2geS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_readme = open(os.path.join(input_data_dir,'readme.txt')).read().split('\\n')\n",
        "\n",
        "print(type(all_readme))\n",
        "\n",
        "# visualize data\n",
        "data_tables = \"\"\n",
        "for line in all_readme:\n",
        "  data_tables += line\n",
        "  if line != all_readme[-1]:\n",
        "    data_tables += \"\\n\"\n",
        "print(data_tables)"
      ],
      "metadata": {
        "id": "I_qit4EjdkHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f67ab880-dd4b-48ee-9bad-4843708d3d39"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "runid      OpSim Name                           ugrizy 5sigma depths\n",
            "1_4_y10    baseline_v1_5_10yrs                  25.86 27.02 26.99 26.42 25.70 24.94\n",
            "4_38_y10   footprint_stuck_rollingv1_5_10yrs    25.56 26.68 26.62 26.06 25.33 24.61\n",
            "10_92_y10  ddf_heavy_nexp2_v1_6_10yrs           25.57 26.82 26.84 26.26 25.57 24.82\n",
            "4_34_y10   footprint_newAv1_5_10yrs             25.75 26.87 26.85 26.29 25.55 24.78\n",
            "7_61_y10   third_obs_pt60v1_5_10yrs             25.87 27.03 26.99 26.43 25.70 24.93\n",
            "9_86_y10   barebones_v1_6_10yrs                 26.00 27.13 27.07 26.57 25.78 25.05\n",
            "\n",
            "test.cat contains the simulated observed apparent magnitudes\n",
            "0     : identifier\n",
            "1     : true redshift\n",
            "2,3   : u, u uncertainty\n",
            "4,5   : g, g uncertainty\n",
            "6,7   : r, r uncertainty\n",
            "8,9   : i, i uncertainty\n",
            "10,11 : z, z uncertainty\n",
            "12,13 : y, y uncertainty\n",
            "14,15 : u-g, u-g uncertainty\n",
            "16,17 : g-r, g-r uncertainty\n",
            "18,19 : r-i, r-i uncertainty\n",
            "20,21 : i-z, i-z uncertainty\n",
            "22,23 : z-y, z-y uncertainty\n",
            "\n",
            "zphot.cat contains the photo-z results\n",
            "0 : identifier\n",
            "1 : true redshift\n",
            "2 : photometric redshift\n",
            "3 : photometric redshift uncertainty\n",
            "4 : number of training-set galaxies in the CMNN subset\n",
            "5 : number of training-set galaxies considered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# individual columns\n",
        "\n",
        "print(all_readme[1].index('25.86')) # is 48\n",
        "\n",
        "data_tableMain = []\n",
        "for lineNum in range(1,7):\n",
        "  line = all_readme[lineNum]\n",
        "  data_tableMain += [line[48:].split( )]\n",
        "print(data_tableMain)\n",
        "\n",
        "# discrete case\n",
        "\n",
        "data_i = []\n",
        "for row in range(len(data_tableMain)):\n",
        "  data_i.append(data_tableMain[row][3])\n",
        "print(data_i)\n",
        "\n",
        "for ind in range(len(data_i)):\n",
        "  data_i[ind] = float(data_i[ind])\n",
        "  data_i[ind] = round(data_i[ind])\n",
        "print(data_i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0DpvanfCuQg",
        "outputId": "689e866e-cd8e-41f7-9fd6-0cf4d1ab3560"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n",
            "[['25.86', '27.02', '26.99', '26.42', '25.70', '24.94'], ['25.56', '26.68', '26.62', '26.06', '25.33', '24.61'], ['25.57', '26.82', '26.84', '26.26', '25.57', '24.82'], ['25.75', '26.87', '26.85', '26.29', '25.55', '24.78'], ['25.87', '27.03', '26.99', '26.43', '25.70', '24.93'], ['26.00', '27.13', '27.07', '26.57', '25.78', '25.05']]\n",
            "['26.42', '26.06', '26.26', '26.29', '26.43', '26.57']\n",
            "[26, 26, 26, 26, 26, 27]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_metadata = []\n",
        "\n",
        "colors = [\"k\", \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "\n",
        "for i, line in enumerate(all_readme[0:6]):\n",
        "  descr = all_readme[i+1].split()\n",
        "  in_metadata.append(descr)\n",
        "\n",
        "  metadatum = namedtuple('metadatum', ['runid', 'OpSimName', 'u', 'g', 'r', 'i', 'z', 'y'])\n",
        "\n",
        "  metadata = {}\n",
        "  for row in in_metadata:\n",
        "    metadata[row[0]] = metadatum(*row)\n",
        "\n",
        "  names_z=('ID', 'z_true', 'z_phot', 'dz_phot', 'NN', 'N_train')\n",
        "  names_phot=('ID', 'z_true',\n",
        "    'u', 'err_u', 'g', 'err_g', 'r', 'err_r', 'i', 'err_i', 'z', 'err_z', 'y', 'err_y',\n",
        "    'u-g', 'err_u-g', 'g-r', 'err_g-r', 'r-i', 'err_r-i', 'i-z', 'err_i-z', 'z-y', 'err_z-y')\n",
        "\n",
        "  available_os = list(metadata.keys())\n",
        "  names = [metadata[runid].OpSimName for runid in available_os]\n",
        "  os_names = dict(zip(available_os, names))\n",
        "  os_colors = dict(zip(available_os, colors))"
      ],
      "metadata": {
        "id": "ZpbQ8cSP9hK5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm the correct file was read and that the mapping from runid to observing strategy name is as expected."
      ],
      "metadata": {
        "id": "C82d2NqT23up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(available_os)\n",
        "print(names)"
      ],
      "metadata": {
        "id": "8QIa6nOzmVjb",
        "outputId": "1f5552fb-186e-46ab-e933-05cd3a4692fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1_4_y10', '4_38_y10', '10_92_y10', '4_34_y10', '7_61_y10', '9_86_y10']\n",
            "['baseline_v1_5_10yrs', 'footprint_stuck_rollingv1_5_10yrs', 'ddf_heavy_nexp2_v1_6_10yrs', 'footprint_newAv1_5_10yrs', 'third_obs_pt60v1_5_10yrs', 'barebones_v1_6_10yrs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define tuples for the redshift and photometric column names in the simulation catalogs."
      ],
      "metadata": {
        "id": "Jvm6Xqaw29gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_z=('ID', 'z_true', 'z_phot', 'dz_phot', 'NN', 'N_train')\n",
        "names_phot=('ID', 'z_true',\n",
        "        'u', 'g', 'r', 'i', 'z', 'y',\n",
        "        'err_u', 'err_g', 'err_r', 'err_i', 'err_z', 'err_y',\n",
        "        'u-g', 'g-r', 'r-i', 'i-z', 'z-y',\n",
        "        'err_u-g', 'err_g-r', 'err_r-i', 'err_i-z', 'err_z-y')\n",
        "\n",
        "\n",
        "#available_os = ['baseline_v1.4_10yrs', 'twilight_neo_mod1_v1.4_10yrs', 'var_expt_v1.4_10yrs', 'weather_1.2_v1.4_10yrs']\n",
        "#os_colors = {'baseline_v1.4_10yrs': 'k', 'twilight_neo_mod1_v1.4_10yrs': '#2ca02c', 'var_expt_v1.4_10yrs': '#1f77b4', 'weather_1.2_v1.4_10yrs': '#ff7f0e'}\n",
        "\n"
      ],
      "metadata": {
        "id": "GeXHGmjKuOoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, read in the photometry and redshift catalogs. Define dictionaries that map the catalogs to the observing strategies."
      ],
      "metadata": {
        "id": "7K23b3Vu3LQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phot_cats, z_cats = {}, {}\n",
        "\n",
        "for name, which_os in zip(names, available_os):\n",
        "  test_cat = Table.read(input_data_dir+'run_'+which_os+'/test.cat', format='ascii')\n",
        "\n",
        "\n",
        "  z_cat = Table.read(input_data_dir+'run_'+which_os+'/zphot.cat',\n",
        "                       format='ascii',\n",
        "                       names=names_z)\n",
        "\n",
        "  phot_cat = Table.read(input_data_dir+'run_'+which_os+'/test.cat',\n",
        "                       format='ascii',\n",
        "                       names=names_phot)\n",
        "  phot_cat = Table.from_pandas(phot_cat.to_pandas().dropna())\n",
        "  phot_cats[which_os] = phot_cat\n",
        "  # print('phot: ')\n",
        "  # print(len(phot_cats[which_os]))\n",
        "  z_cats[which_os] = z_cat\n",
        "  # print('z: ')\n",
        "  # print(len(z_cats[which_os]))"
      ],
      "metadata": {
        "id": "Huy8pnD3zAFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_cats_no_nan = {}\n",
        "for which_os in available_os:\n",
        "  IDS = phot_cats[which_os]['ID']\n",
        "  z_cats_no_nan[which_os] = z_cats[which_os].to_pandas()[z_cats[which_os].to_pandas()['ID'].isin(IDS)]"
      ],
      "metadata": {
        "id": "tXcCTuICAnle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # view the photometric catalog for one observing strategy\n",
        "\n",
        "# phot_cats[which_os]"
      ],
      "metadata": {
        "id": "rVUYIhsGNXPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the input redshift distributions n(z) for each runID/observing strategy."
      ],
      "metadata": {
        "id": "Zpv07qYL3rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from matplotlib.pyplot import hist, xlabel, ylabel, legend\n",
        "\n",
        "# zbins = np.linspace(0, 3, 300)\n",
        "\n",
        "# for which_os in available_os:\n",
        "#   hist(z_cats[which_os]['z_true'], bins=zbins, alpha=0.75, histtype=\"step\", color=os_colors[which_os],\n",
        "#        label=os_names[which_os], density = False)\n",
        "# xlabel(r'true redshift $z$')\n",
        "# ylabel('number of galaxies')\n",
        "# legend(loc='upper right', fontsize='small')"
      ],
      "metadata": {
        "id": "7FnoZxdwu9uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots(1,6, figsize=(24,5), sharey=True)\n",
        "# for which_os in available_os:\n",
        "#   for b, datum in enumerate(['u-g', 'g-r', 'r-i', 'i-z', 'z-y']):\n",
        "#     ax[b].hist(phot_cats[which_os][datum], alpha=0.9, histtype=\"step\", color=os_colors[which_os],\n",
        "#        label=os_names[which_os], density = False)\n",
        "#     ax[b].set_xlabel(datum, fontsize=14)\n",
        "#   ax[-1].hist(phot_cats[which_os]['r'], alpha=0.9, histtype=\"step\", color=os_colors[which_os],\n",
        "#        label=os_names[which_os], density = False)\n",
        "# ax[-1].set_xlabel('r', fontsize=14)\n",
        "# ax[0].set_ylabel('number of galaxies', fontsize=14)\n",
        "# ax[-1].legend(loc='upper left', fontsize='medium')\n",
        "# plt.tight_layout()"
      ],
      "metadata": {
        "id": "wLrj0fT9-FM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['z_true', 'u-g', 'g-r', 'r-i', 'i-z', 'z-y', 'r']\n",
        "def prep_for_corner(one_os):\n",
        "  return np.array([phot_cats[one_os]['z_true'], phot_cats[one_os]['u-g'],\n",
        "                   phot_cats[one_os]['g-r'], phot_cats[one_os]['r-i'],\n",
        "                   phot_cats[one_os]['i-z'], phot_cats[one_os]['z-y'],\n",
        "                   phot_cats[one_os]['r']]).T\n",
        "fig = corner.corner(prep_for_corner(available_os[0]), labels=labels, fontsize=14)\n",
        "for which_os in available_os[1:]:\n",
        "  corner.corner(prep_for_corner(which_os), fig=fig, color=os_colors[which_os], fontsize=14)"
      ],
      "metadata": {
        "id": "pwjfkV3hBw18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new experimentation"
      ],
      "metadata": {
        "id": "aMnDngkZevAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SNu3sl3espc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print data in columns\n",
        "# histograms of columns\n",
        "# pick a column to copy (r)\n",
        "# round to nearest whole number\n",
        "# add a copy of it to the astropy table"
      ],
      "metadata": {
        "id": "R05V3xADd3x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to define the notion of a 'conditional column', that is, p(z | X), where X are the photometric columns and z is the conditional column that is conditioned on the phtometry."
      ],
      "metadata": {
        "id": "F6IB8aPo3yDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_cols = ['ID', 'z_true', 'u', 'g',  'i', 'z', 'y',\n",
        "        'err_u', 'err_g', 'err_r', 'err_i', 'err_z', 'err_y',\n",
        "        'err_u-g', 'err_g-r', 'err_r-i', 'err_i-z', 'err_z-y']\n"
      ],
      "metadata": {
        "id": "JRp8uTEKzAKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip the next two cells if loading pre-trained flows**"
      ],
      "metadata": {
        "id": "JO-azU4TkMW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now loop through the observing strategies and define a dictionary containing ensembles of normalizing flow objects that will be trained on the simulated data."
      ],
      "metadata": {
        "id": "sReXBKEz4BdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # mins = [0, 20, -5, -5, -5, -5, -5]\n",
        "# # maxs = [6, 30, 5, 5, 5, 5, 5]\n",
        "\n",
        "# # the data column is the one that is sampled and transformed by the flow\n",
        "# data_columns = [\"z_true\"]\n",
        "\n",
        "# all_cond_cols = {}\n",
        "# ensembles = dict()\n",
        "# K = 16\n",
        "\n",
        "# for which_os in available_os:#catalogs.keys():\n",
        "#  # the conditional columns are the columns that the flow is conditioned on\n",
        "#   conditional_columns = phot_cats[which_os].to_pandas().drop(drop_cols, axis = 1)\n",
        "#   all_cond_cols[os] = conditional_columns\n",
        "#   # print((os, conditional_columns))\n",
        "#   ndcol = len(data_columns)\n",
        "#   ncond = len(conditional_columns)\n",
        "#   ndim = ndcol+ncond\n",
        "\n",
        "# # first I create a bijector chain\n",
        "# # the first bijection is a standard scaler - but I'm not actually using it for standard scaling,\n",
        "# # the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "# #     which will be the r mag and the galaxy colors\n",
        "\n",
        "\n",
        "#   bijector = Chain(\n",
        "#     StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "#     NeuralSplineCoupling(B=5, n_conditions=6, K=K)\n",
        "#   )\n",
        "\n",
        "# # I set the latent distribution to a Uniform over (-5, 5)\n",
        "# # this range was chosen to match the NeuralSplineCoupling domain\n",
        "# # I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "\n",
        "#   latent = Uniform(input_dim=ndcol, B=7) # this has changed.\n",
        "\n",
        "#   info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K = 16\"\n",
        "\n",
        "#   flowEns2 = FlowEnsemble(data_columns = data_columns,\n",
        "#                            conditional_columns = conditional_columns,\n",
        "#                            bijector = bijector,\n",
        "#                            latent = latent,\n",
        "#                            info = info,\n",
        "#                            N = 10\n",
        "#                             )\n",
        "#   ensembles[which_os] = flowEns2"
      ],
      "metadata": {
        "id": "jjM40GXtwR9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next cell trains the model for 150 epochs and saves the output."
      ],
      "metadata": {
        "id": "cN96gP0Z5mkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # tav_train, tav_test = {}, {}\n",
        "# # available_os = ['weather_1.2_v1.4_10yrs']\n",
        "\n",
        "# for which_os in available_os:#:\n",
        "#     ens  = ensembles[which_os]\n",
        "#     # get the data and make a train and test set\n",
        "#     cat = phot_cats[which_os].to_pandas()\n",
        "#     # cat_train = cat.sample(frac = 0.99)\n",
        "#     # cat_test = cat.drop(cat_train.index)\n",
        "#     cat_train = cat\n",
        "#     print(which_os)\n",
        "#     # train the flow on the given learning rate schedule\n",
        "#     loss = ens.train(cat_train,#conditional_columns],\n",
        "#                                convolve_errs=False,\n",
        "#                        epochs = 150, verbose=True)\n",
        "#     # loss2 = ens.train(cat_train, convolve_errs=True,\n",
        "#     #                    epochs = 30, seed = 312)\n",
        "#     # loss3 = ens.train(cat_train, convolve_errs=True,\n",
        "#     #                    epochs = 30, seed = 231)\n",
        "\n",
        "#     # losses = {fname : # for each flow trained in the ensemble...\n",
        "#     #               [float(loss) # save the list of training losses\n",
        "#     #                for lossDict in [loss1]#, loss2, loss3]\n",
        "#     #                for loss in lossDict[fname]]\n",
        "#     #           for fname in loss1}\n",
        "\n",
        "#     # print the train and test loss\n",
        "#     train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "#     # test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "#     print(which_os, train_loss)#, test_loss)\n",
        "#     # post_trained[os] = ens\n",
        "#     # save the ensemble\n",
        "#     ens.save(prepend+f\"pzflow_ensemble_for_{which_os}.pkl\")\n",
        "#     # and the losses\n",
        "#     with open(prepend+f\"losses_for_{which_os}.pkl\", \"wb\") as file:\n",
        "#         pickle.dump({\"losses\": loss,\n",
        "#                      \"train loss\": train_loss,\n",
        "#                     #  \"test loss\": test_loss,\n",
        "#                      \"train_ids\": cat_train.index},\n",
        "#                     file)\n",
        "\n",
        "#     # mutual_information_lower_bound_train = ens.log_prob(cat_train, returnEnsemble=True)\n",
        "#     # mutual_information_lower_bound_test = ens.log_prob(cat_test, returnEnsemble=True)\n",
        "#     # tav_train[os] = mutual_information_lower_bound_train\n",
        "#     # tav_test[os] = mutual_information_lower_bound_test\n"
      ],
      "metadata": {
        "id": "j7MYPD_q00Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, we open the saved flows by looping through a dictionary defining a mapping between observing strategy and each flow ensemble object. And we visualize the losses to confirm convergence."
      ],
      "metadata": {
        "id": "gvvfzqhG50XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change to the Directory where the flows are saved.\n",
        "# cd /content/drive/MyDrive/Research/RubinPz/trained_flows_losses_May\n",
        "prepend = 'drive/MyDrive/Research/RubinPz/trained_flows_losses_May/'"
      ],
      "metadata": {
        "id": "PR7pjsqQ_p__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# why no Flow 0 in losses? but it's present in the flows which matters more\n",
        "\n",
        "flows = {}\n",
        "for which_os in available_os:\n",
        "  flows[which_os] = [FlowEnsemble(file=prepend+f\"pzflow_ensemble_for_{which_os}.pkl\" )]\n",
        "  with open(prepend+f'losses_for_'+which_os+'.pkl', \"rb\") as file:\n",
        "    losses = pickle.load(file)[\"losses\"]\n",
        "    # print(np.shape(losses))\n",
        "  fig, ax = plt.subplots(figsize=(6, 4.25), dpi=100)\n",
        "  ax.set_title(os_names[which_os])\n",
        "  for k in losses.keys():\n",
        "    ax.plot(losses[k], alpha=0.5)#, label = str(i))\n",
        "    # ax.plot([], c=os_colors['10_92_y10'], label=i)\n",
        "\n",
        "# ax.legend()\n",
        "  ax.set(xlabel=\"Epoch\", ylabel=\"Training loss\")\n",
        "  plt.show()\n",
        "\n",
        "# just_tav = {}\n",
        "# for os in available_os:\n",
        "#   just_tav[os] = post_trained[os].log_prob(catalogs[os])\n",
        "\n",
        "flows"
      ],
      "metadata": {
        "id": "rZWOOR4qdsiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots(figsize=(6, 4.25), dpi=100)\n",
        "\n",
        "\n",
        "# with open(f\"losses_for_10_92_y10.pkl\", \"rb\") as file:\n",
        "#     losses = pickle.load(file)[\"losses\"]\n",
        "\n",
        "# for i,ls in enumerate(losses.values()):\n",
        "#     ax.plot(ls, alpha=0.5, label = str(i))\n",
        "#     # ax.plot([], c=os_colors['10_92_y10'], label=i)\n",
        "\n",
        "# ax.legend()\n",
        "# ax.set(xlabel=\"Epoch\", ylabel=\"Training loss\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "h-q4cOEA6wMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next, we compute the per galaxy log probability and aggregate those into TheLastMetric. This requires us to loop over the observing strategies, each of which is stored in flows[which_os]."
      ],
      "metadata": {
        "id": "6JdPoYv865P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_logp = {}\n",
        "# for which_os in available_os:\n",
        "#   all_logp[which_os] = np.stack([f.log_prob(phot_cats[which_os].to_pandas(), returnEnsemble=True)\n",
        "#                                  for f in flows[which_os] ], axis=0)"
      ],
      "metadata": {
        "id": "z1m4tb71f5vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zbins = np.linspace(0, 3, 32)\n",
        "# avgtav = {}\n",
        "# errtav = {}\n",
        "# for which_os in available_os:\n",
        "#   plotran = np.empty_like(zbins)\n",
        "#   ploterr = np.empty_like(zbins)\n",
        "#   if 'logprob' not in phot_cats[which_os].columns.values():\n",
        "#     phot_cats[which_os]['logprob'] = all_logp[which_os]\n",
        "#   for i, zmin in enumerate(zbins[:-1]):\n",
        "#     zmax = zbins[i+1]\n",
        "#     subset = phot_cats[which_os][(phot_cats[which_os]['z_true'] >= zmin) & (phot_cats[which_os]['z_true'] < zmax)]\n",
        "#     # print((zmin, len(subset), zmax))\n",
        "#     vals = subset['logprob']\n",
        "#     mean = np.mean(vals)\n",
        "#     std = np.std(vals)\n",
        "#     plotran[i] = mean\n",
        "#     ploterr[i] = std\n",
        "#   avgtav[os] = plotran\n",
        "#   errtav[os] = ploterr\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_HM2gmJq_LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for which_os in available_os:\n",
        "#   plt.errorbar(zbins, avgtav[which_os], errtav[which_os], label=which_os, alpha=0.5)\n",
        "# plt.legend()\n",
        "# plt.xlabel('z_true')\n",
        "# plt.ylabel(chr(0x05ea))"
      ],
      "metadata": {
        "id": "k_oqYokRnh-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_last_metric(flow, photometry, redshift,\n",
        "#                         entropy_nbins=120,\n",
        "#                         entropy_range=[0.,3.]):\n",
        "#   \"\"\" Computes the last metric given a trained flow and corresponding photometry\n",
        "#   and redshift astropy tables\n",
        "#   \"\"\"\n",
        "#   cat = photometry.to_pandas().merge(redshift.to_pandas())\n",
        "\n",
        "#   # Computing the entropy H(z)\n",
        "#   pz = scipy.stats.rv_histogram(np.histogram(cat['z_true'], bins=entropy_nbins,\n",
        "#                                 range=entropy_range))\n",
        "#   entropy = pz.entropy()\n",
        "\n",
        "#   # Computing lower bound\n",
        "#   mutual_information_lower_bound = flow.log_prob(cat) + entropy\n",
        "\n",
        "#   return mutual_information_lower_bound"
      ],
      "metadata": {
        "id": "ESCMjRWb087G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=101\n",
        "b = sps.mstats.mquantiles(z_cats_no_nan[which_os]['z_true'], np.linspace(0,1,n, endpoint=True))\n",
        "# print(len(z_cats_no_nan[which_os]['z_true']))\n",
        "# print(b)\n",
        "\n",
        "b_centers = 0.5*(b[1:] + b[:-1])\n",
        "db = b[1:] - b[:-1]"
      ],
      "metadata": {
        "id": "VCNEZC-7PjPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xvPZkZWAZHvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import scipy.stats as sps\n",
        "\n",
        "# dictionary to hold all the MILB\n",
        "all_tlm = {}\n",
        "all_milb = {}\n",
        "all_ent = {}\n",
        "\n",
        "for os in available_os:\n",
        "    # print(os_names[os])\n",
        "    # load the photometric catalog for this os\n",
        "    cat = phot_cats[os].to_pandas().merge(z_cats[os].to_pandas())\n",
        "\n",
        "  # Computing the entropy H(z)\n",
        "    pz = sps.rv_histogram(np.histogram(phot_cats[os]['z_true'], bins=b))\n",
        "    entropy = pz.entropy()\n",
        "    # just checking that this isn't discrete entropy and that binning doesn't need to match anything else\n",
        "    # testent = sum(-1 * pz.logpdf(b_centers) * pz.pdf(b_centers) * db)\n",
        "    # print(entropy - testent)\n",
        "    all_ent[os] = entropy\n",
        "\n",
        "  # Computing lower bound\n",
        "    # all_milb = []\n",
        "    # print(len(flows[os]))\n",
        "    # for i in range(10):\n",
        "    milb = flows[os][0].log_prob(cat, returnEnsemble=True, err_samples=10)# + entropy\n",
        "    # print(milb)\n",
        "    all_milb[os] = np.array(milb)\n",
        "    # print(np.shape(all_milb[os]))\n",
        "    all_tlm[os] = milb.mean(axis=0) + entropy\n",
        "    # print(all_tlm[os])\n",
        "    # all_os_milb[os] = np.array(all_milb)\n",
        "    # print((all_milb[os].shape, all_tlm[os].shape))\n",
        "\n",
        "#TODO: actually need to add these errors in quadrature\n",
        "    print((os_names[os], np.mean(all_tlm[os]), np.std(all_tlm[os])))\n",
        "\n",
        "# all_os_milb = all_milb\n",
        "\n",
        "with open(prepend+'stats.pkl', 'wb') as statfile:\n",
        "  pickle.dump({'tlm': all_tlm, 'milb': all_milb, 'ent': all_ent}, statfile)"
      ],
      "metadata": {
        "id": "hq6lSqlzLJIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(prepend+'stats.pkl', 'rb') as statfile:\n",
        "  statdict = pickle.load(statfile)"
      ],
      "metadata": {
        "id": "KEPtXwreWDU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tlm = statdict['tlm']\n",
        "all_milb = statdict['milb']\n",
        "all_ent = statdict['ent']"
      ],
      "metadata": {
        "id": "rsHrRSijWMFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_milb[os].shape"
      ],
      "metadata": {
        "id": "O17JwPr-X5QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for os in available_os:\n",
        "  plt.hist(all_tlm[os].mean(axis=1), color=os_colors[os])\n",
        "  plt.hist(all_milb[os].mean(axis=1), color=os_colors[os])"
      ],
      "metadata": {
        "id": "gxKB1DP1W823"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for os in available_os:\n",
        "#   all_tlm_err[os] = np.sqrt(np.sum(all_milb[os].var(axis=0)))"
      ],
      "metadata": {
        "id": "t3yD9AI_Yntc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Computing metric for each observing strategy\n",
        "# all_tlm = {}\n",
        "# for which_os in available_os:\n",
        "#   all_tlm[which_os] = np.stack([(compute_last_metric(f,\n",
        "#                                           phot_cats[which_os],\n",
        "#                                           z_cats[which_os], entropy_nbins=60)) for f in flows[which_os] ], axis=0)\n",
        "#   print((os_names[which_os], np.mean(all_tlm[which_os]), np.std(np.mean(all_tlm[which_os], axis=1))))\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1xF9ro91FSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Histogram of TLM values for each observing strategy"
      ],
      "metadata": {
        "id": "-gAcANn_AEmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_tavbins = np.linspace(-5, 5, 100)\n",
        "\n",
        "for os in available_os:\n",
        "  # bt = sps.mstats.mquantiles(all_tlm[os].flatten(), np.linspace(0,1,n, endpoint=True))\n",
        "  # print(all_os_milb[os][0].shape)\n",
        "  plt.hist(all_milb[os].flatten(), density=True, alpha=0.25,\n",
        "           label=os_names[os], color=os_colors[os],\n",
        "           bins=overall_tavbins)\n",
        "plt.legend()\n",
        "# plt.xlim(-5,4)\n",
        "plt.xlabel(r'$\\log q_{\\varphi} (z | x_{phot})$', fontsize=12)#r'$\\mathbb{E}_{z, x_{phot}} \\left[ q_\\theta(z | x_{phot}) \\right]$')\n",
        "plt.yticks([])\n",
        "# plt.savefig(prepend+'all_combos_histogram.pdf')\n",
        "plt.ylabel(r'probability density $p(\\log q_{\\varphi} (z | x_{phot}))$', fontsize=12)"
      ],
      "metadata": {
        "id": "1lJRcA4tuuFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavbins = np.linspace(2.875, 3.15, 300)"
      ],
      "metadata": {
        "id": "vnWxQgluJh1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_mu = {}\n",
        "all_sig = {}\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "for which_os in available_os:\n",
        "  # plt.hist(all_tlm[which_os],#np.mean([(all_logp[which_os].T[j]) for j in range(10)], axis=1),\n",
        "  #          bins=tavbins, alpha=0.6,\n",
        "  #      color=os_colors[which_os], label=os_names[which_os])\n",
        "\n",
        "  # for j in range(10):\n",
        "  #   plt.axvline(np.mean(all_logp[which_os].T[j]), alpha=0.6,\n",
        "  #      color=os_colors[which_os])\n",
        "  mu = np.mean(all_tlm[which_os])\n",
        "  all_mu[os] = mu\n",
        "  sig = np.std(all_tlm[which_os])\n",
        "  all_sig[os] = sig\n",
        "  plt.axvline(mu, linewidth=2, color=os_colors[which_os], label=os_names[which_os])\n",
        "  plt.vlines(all_tlm[which_os], 0, 2.5, color=os_colors[which_os], linewidth=1)\n",
        "  plt.fill_between(tavbins, sps.norm(mu, sig).pdf(tavbins),\n",
        "                   alpha=0.3, color=os_colors[which_os])\n",
        "plt.legend(ncol=2, fontsize='small')\n",
        "plt.xlabel(chr(0x05ea), fontsize=16)\n",
        "plt.ylim(0, 40)\n",
        "plt.yticks([])\n",
        "plt.ylabel(r'probability density $p($'+chr(0x05ea)+'$)$', fontsize=14)\n",
        "# plt.savefig('metrics.pdf', bbox_inches = 'tight', pad_inches = 0 )"
      ],
      "metadata": {
        "id": "qYxeo3rn1SYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison to traditional metrics pf photo-z estimates"
      ],
      "metadata": {
        "id": "iIR9vZxHyUpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cmnn_bias(which_os):\n",
        "    bias = (z_cats[which_os]['z_phot'] - z_cats[which_os]['z_true']) / (1 + z_cats[which_os]['z_phot'])\n",
        "    return np.mean(bias)\n",
        "\n",
        "def cmnn_stdd(which_os):\n",
        "    stdd = sps.iqr((z_cats[which_os]['z_phot'] - z_cats[which_os]['z_true']) / (1 + z_cats[which_os]['z_phot']), scale='normal')\n",
        "    return stdd#np.sqrt(np.mean(stdd))\n",
        "\n",
        "def cmnn_out(which_os):\n",
        "    stdd = cmnn_stdd(which_os)\n",
        "    outs = z_cats[which_os][np.abs(z_cats[which_os]['z_phot'] - z_cats[which_os]['z_true']) >= 3.*stdd * (1 + z_cats[which_os]['z_phot'])]\n",
        "    return len(outs) / len(z_cats[which_os])\n",
        "\n",
        "\n",
        "\n",
        "all_mets = {}\n",
        "tlm_over_trainings = {}\n",
        "std_tlm_over_trainings = {}\n",
        "for which_os in available_os:\n",
        "    all_mets[which_os] = {}\n",
        "    all_mets[which_os]['CMNN bias'] = cmnn_bias(which_os)\n",
        "    all_mets[which_os]['CMNN scatter'] = cmnn_stdd(which_os)\n",
        "    all_mets[which_os]['CMNN outliers'] = cmnn_out(which_os)\n",
        "#     all_mets[which_os]['TLM mean'] = np.mean(all_milb[which_os].flatten())\n",
        "#     all_mets[which_os]['TLM stdd'] = np.std(all_milb[which_os].flatten())\n",
        "#     for key in ['CMNN bias', 'CMNN scatter', 'CMNN outliers']:#, 'TLM mean']:#, 'TLM stdd']:\n",
        "#         extrema[key].append(all_mets[which_os][key])\n",
        "    tlm_over_trainings[os_names[which_os]] = np.mean(all_tlm[which_os])\n",
        "    std_tlm_over_trainings[os_names[which_os]] = np.std(all_tlm[which_os])\n",
        "#     print((os_names[which_os], all_mets[which_os]))\n",
        "\n"
      ],
      "metadata": {
        "id": "xBnDHF1csDUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=(5, 7.5), sharex=True)\n",
        "# x = [tlm_over_trainings[os_names[which_os]] for which_os in available_os]\n",
        "# xerr = [std_tlm_over_trainings[os_names[which_os]] for which_os in available_os]\n",
        "axs[-1].set_xlabel(r'$\\langle$'+chr(0x05ea)+r'$\\rangle$', fontsize=14)\n",
        "for which_os in available_os:\n",
        "    x = tlm_over_trainings[os_names[which_os]]\n",
        "    xerr = std_tlm_over_trainings[os_names[which_os]]\n",
        "    for i, met in enumerate(['CMNN scatter', 'CMNN outliers']):\n",
        "        y = all_mets[which_os][met]\n",
        "        axs[i].errorbar(x, y, xerr=xerr, capsize=5,\n",
        "                        color=os_colors[which_os], fmt='o')\n",
        "for i, met in enumerate(['CMNN scatter', 'CMNN outliers']):\n",
        "    axs[i].set_ylabel(met, fontsize=14)\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(hspace=0.0)\n",
        "\n"
      ],
      "metadata": {
        "id": "LPaf2dgqsl2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# And compute the average value of the last metric in redshift bins."
      ],
      "metadata": {
        "id": "04IhknU9AepY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_logp = {}\n",
        "# for which_os in available_os:\n",
        "#   all_logp[which_os] = np.stack([f.log_prob(phot_cats[which_os].to_pandas())\n",
        "#                                  for f in flows[which_os] ], axis=0)\n",
        "\n",
        "# from scipy import stats\n",
        "nz=31\n",
        "bz = sps.mstats.mquantiles(z_cats_no_nan[which_os]['z_true'], np.linspace(0, 1, nz, endpoint=True))\n",
        "# print(len(z_cats_no_nan[which_os]['z_true']))\n",
        "\n",
        "mis = {}\n",
        "\n",
        "for which_os in available_os:\n",
        "  inds = np.digitize(z_cats_no_nan[which_os]['z_true'], bz) -1\n",
        "\n",
        "  print(len(inds))\n",
        "\n",
        "  all_res = []\n",
        "  for j in range(10):\n",
        "    res = np.zeros(nz-1)\n",
        "    for i in range(nz-1):\n",
        "      # res[i] = np.mean( np.mean((all_logp[which_os].flatten()[inds == i])))\n",
        "      res[i] = np.mean(all_milb[which_os].T[j][inds == i])\n",
        "    all_res.append(res)\n",
        "  mis[which_os] = np.array(all_res)\n"
      ],
      "metadata": {
        "id": "QsWIMD4f1ruc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bz_centers = 0.5*(bz[1:] + bz[:-1])\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "for which_os in available_os:\n",
        "  for j in range(10):\n",
        "    plt.plot(bz_centers, -mis[which_os][j], color=os_colors[which_os], alpha=0.25, linewidth=0.5)\n",
        "  plt.plot(bz_centers, np.mean(-mis[which_os], axis=0), label=os_names[which_os], color=os_colors[which_os], alpha=0.75)\n",
        "plt.legend()\n",
        "plt.ylabel(r'$- \\log q_{\\varphi} (z | x_{phot})$', fontsize=14)\n",
        "plt.xlabel('True redshift z', fontsize=14)\n",
        "plt.xlim(0,3);\n",
        "# TODO: fix y axis: this is -MILB, including entropy and as f(z)"
      ],
      "metadata": {
        "id": "WbOBke5X1b9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,5))\n",
        "for which_os in available_os:\n",
        "  sig = np.std(-mis[which_os], axis=0)\n",
        "  # for j in range(10):\n",
        "  #   plt.plot(b_centers, -mis[which_os][j], color=os_colors[which_os], alpha=0.25, linewidth=0.5)\n",
        "  mu = np.mean(-mis[which_os], axis=0)\n",
        "  plt.plot(bz_centers, mu, label=os_names[which_os], color=os_colors[which_os], alpha=0.75)\n",
        "  plt.fill_between(bz_centers, mu - sig, mu + sig, color=os_colors[which_os], alpha=0.25)\n",
        "plt.legend()\n",
        "plt.ylabel(r'$- \\log q_{\\varphi} (z | x_{phot})$', fontsize=14)\n",
        "plt.xlabel('True redshift z', fontsize=14)\n",
        "plt.xlim(0,3);\n",
        "# TODO: fix y axis: this is -MILB, including entropy and as f(z)"
      ],
      "metadata": {
        "id": "55-gcIYL6osq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # all_logp = {}\n",
        "# # for which_os in available_os:\n",
        "# #   all_logp[which_os] = np.stack([f.log_prob(phot_cats[which_os].to_pandas())\n",
        "# #                                  for f in flows[which_os] ], axis=0)\n",
        "\n",
        "# from scipy import stats\n",
        "# n=32\n",
        "# b = stats.mstats.mquantiles(z_cats_no_nan[which_os]['z_true'], np.linspace(0,1,n, endpoint=True))\n",
        "# print(len(z_cats_no_nan[which_os]['z_true']))\n",
        "\n",
        "# var2 = {}\n",
        "\n",
        "# for which_os in available_os:\n",
        "#   inds = np.digitize(z_cats_no_nan[which_os]['z_true'], b) -1\n",
        "\n",
        "#   print(len(inds))\n",
        "\n",
        "\n",
        "#   vars = np.zeros(n-1)\n",
        "#   for i in range(n-1):\n",
        "#     # res[i] = np.mean( np.mean((all_logp[which_os].flatten()[inds == i])))\n",
        "#     vars[i] = np.mean( np.var([(all_logp[which_os].T[j].flatten()[inds == i]) for j in range(10)]))\n",
        "\n",
        "#   var2[which_os] = np.sqrt(vars)\n",
        "\n",
        "# b_centers = 0.5*(b[1:] + b[:-1])\n",
        "\n",
        "# plt.figure(figsize=(6,5))\n",
        "# for i, which_os in enumerate(available_os):\n",
        "#   plt.plot(b_centers, - mis[which_os], label=os_names[which_os], color=os_colors[which_os])\n",
        "#   plt.fill_between(b_centers, - mis[which_os] - var2[which_os], - mis[which_os] + var2[which_os], color=os_colors[which_os], edgecolor=os_colors[which_os], alpha = 0.1)\n",
        "# plt.legend()\n",
        "# plt.ylabel(r'$- \\log q_{\\varphi} (z | x_{phot})$')\n",
        "# plt.xlabel('True redshift z')\n",
        "# plt.xlim(0,3);"
      ],
      "metadata": {
        "id": "4Qkiq4qt1QiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Same as before, with baseline scenario subtracted off."
      ],
      "metadata": {
        "id": "10dOk-7hBJpH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dq87gs3m4sDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for which_os in available_os:\n",
        "#   plt.plot(b_centers, mis[which_os] - mis['1_4_y10'], label=os_names[which_os], color=os_colors[which_os])\n",
        "# plt.legend(loc='upper right')\n",
        "# plt.xlabel('$z$')\n",
        "# plt.ylabel('$I(X ; Z) - I(X_{baseline} ; Z)$');\n",
        "# plt.savefig('figure4.pdf')\n",
        "# plt.ylim(-0.6,0.6)"
      ],
      "metadata": {
        "id": "mR20dFubuAG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YomDn4vK1E6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}